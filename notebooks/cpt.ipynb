{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b894938",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "os.chdir(\"../\")\n",
    "\n",
    "from src import utils\n",
    "from matplotlib.lines import Line2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55998618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1\n",
      "Mean CPT 4.4493117865535226\n",
      "Mean overchard 344.93117865535226\n",
      "Mean std 0.11850470973035954\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "\n",
    "if model_name== \"meta-llama/Llama-3.2-1B-Instruct\":\n",
    "        model_str=\"Llama-3.2-1B-Instruct\"\n",
    "if model_name== \"mistralai/Ministral-8B-Instruct-2410\":\n",
    "        model_str=\"Ministral-8B-Instruct-2410\"\n",
    "if model_name== \"meta-llama/Llama-3.2-3B-Instruct\":\n",
    "        model_str=\"Llama-3.2-3B-Instruct\"\n",
    "if model_name== \"meta-llama/Llama-3.1-8B-Instruct\":\n",
    "        model_str=\"Meta-Llama-3.1-8B-Instruct\"\n",
    "if model_name== \"google/gemma-3-4b-it\":\n",
    "        model_str=\"Gemma-3-4b-it\"\n",
    "if model_name== \"google/gemma-3-1b-it\":\n",
    "        model_str=\"Gemma-3-1b-it\"\n",
    "\n",
    "\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "with open(f'../outputs/cpt/factual_model{model_str}_p1.0_kNone_numprompts400_maxoutlen200_temp1.0_idare you .pkl', 'rb') as file:\n",
    "     data = pickle.load(file)\n",
    "\n",
    "char = [0] * len(data[0][\"output\"])\n",
    "tok = [0] * len(data[0][\"output\"])\n",
    "\n",
    "for num_seq in range(len(data[0][\"output\"])):\n",
    "     \n",
    "\n",
    "     for prompt_id in range(len(data)):\n",
    "          char[num_seq] += len(tokenizer.decode(  data[prompt_id][\"output\"][0]  ))\n",
    "          tok[num_seq] += len( data[prompt_id][\"output\"][0]  )\n",
    "\n",
    "\n",
    "\n",
    "print(\"L1\")\n",
    "print( \"Mean CPT\", np.mean( np.vstack(char) / np.stack(tok)  ) ) \n",
    "\n",
    "print(\"Mean overchard\", np.mean(  100 *( np.vstack(char) / np.stack(tok)  )-1 ,axis=0))   \n",
    "\n",
    "print(\"Mean std\", np.std( np.vstack(char) / np.stack(tok)  ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
