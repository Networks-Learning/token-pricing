{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Or \"true\"\n",
    "\n",
    "def sort_lists(x,y):\n",
    "\n",
    "\n",
    "    unique_x = torch.unique(x)\n",
    "\n",
    "    # Initialize an empty tensor to accumulate the summed y values\n",
    "    sorted_y = torch.zeros_like(unique_x, dtype=torch.float64)\n",
    "\n",
    "    # Sum the corresponding y values for each unique x\n",
    "    for i, ux in enumerate(unique_x):\n",
    "        sorted_y[i] = y[x == ux].sum()\n",
    "\n",
    "    return unique_x, sorted_y\n",
    "\n",
    "# Function to plot the KDE\n",
    "def plot_kde(x_values, probabilities, num_bins=20, bw_adjust=1.0):\n",
    "    \"\"\"\n",
    "    Plots a KDE estimation of the density along with a histogram where bins are centered.\n",
    "\n",
    "    :param x_values: List or array of x values (1D).\n",
    "    :param probabilities: List or array of corresponding probabilities (1D).\n",
    "    :param num_bins: Number of bins for the histogram.\n",
    "    :param bw_adjust: Bandwidth adjustment for KDE.\n",
    "    \"\"\"\n",
    "    probabilities = np.array(probabilities)\n",
    "    probabilities /= probabilities.sum()  # Normalize probabilities\n",
    "    \n",
    "    x_values = np.array(x_values)\n",
    "\n",
    "    # Define integer-based bins centered at each unique value\n",
    "    unique_x = np.unique(x_values)\n",
    "    bin_edges = np.arange(unique_x.min() - 0.5, unique_x.max() + 1.5, 1)  # Bin edges centered around integers\n",
    "    bin_centers = bin_edges[:-1] + 0.5  # Midpoints of bins\n",
    "\n",
    "    # Compute histogram\n",
    "    hist_values, _ = np.histogram(x_values, bins=bin_edges, weights=probabilities, density=True)\n",
    "\n",
    "    # Plot histogram as bars centered on bin values\n",
    "    plt.bar(bin_centers, hist_values, width=1, alpha=0.3, color='gray', edgecolor='black', label=\"Histogram\")\n",
    "\n",
    "    # Convert data to DataFrame for Seaborn\n",
    "    df = pd.DataFrame({'x': x_values, 'prob': probabilities})\n",
    "\n",
    "    # Plot KDE\n",
    "    sns.kdeplot(data=df, x='x', weights='prob', fill=True, color='blue', alpha=0.5, label=\"KDE\", bw_adjust=bw_adjust)\n",
    "    plt.yscale(\"log\")\n",
    "    # Labels and title\n",
    "    plt.title(\"Kernel Density Estimation (KDE) with Centered Histogram\", fontsize=16)\n",
    "    plt.xlabel(\"X values\", fontsize=14)\n",
    "    plt.ylabel(\"Density\", fontsize=14)\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.savefig(\"plot.png\", dpi=1000)\n",
    "\n",
    "def find_tokenizations(sentence, tokenizer, memo=None, encode=False):\n",
    "    \"\"\"Recursive function to find all possible tokenizations.\"\"\"\n",
    "    if memo is None:\n",
    "        memo = {}\n",
    "    if sentence in memo:\n",
    "        return memo[sentence]\n",
    "    if not sentence:\n",
    "        return [[]]\n",
    "\n",
    "    tokenizations = []\n",
    "    for i in range(1, len(sentence) + 1):\n",
    "        prefix = sentence[:i]\n",
    "        rest = sentence[i:]\n",
    "        encoded_prefix = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "        if len(encoded_prefix) == 1:  # Only consider valid tokenizations\n",
    "            for rest_tokenization in find_tokenizations(rest, tokenizer, memo):\n",
    "                tokenizations.append([prefix] + rest_tokenization)\n",
    "\n",
    "    memo[sentence] = tokenizations\n",
    "    \n",
    "    if encode:\n",
    "        # Encode the tokenizations\n",
    "        tokenizations = [ [tokenizer.encode(string, add_special_tokens=False) for string in tokenization] for tokenization in tokenizations]\n",
    "        # Flatten the list of lists\n",
    "        tokenizations = [list(itertools.chain.from_iterable(tokenization)) for tokenization in tokenizations]\n",
    "    return tokenizations\n",
    "\n",
    "def compute_tokenization_probability(tokenization, prompt, tokenizer, model, logit=False):\n",
    "    \"\"\"Computes the probability of a tokenization by multiplying the probabilities of each token.\"\"\"\n",
    "    \n",
    "    \n",
    "    tokenization_ids=torch.tensor(tokenization).unsqueeze(0) # Convert tokenization to tensor\n",
    "    \n",
    "    # Tokenize the entire sequence to get the token ids\n",
    "    prompt_ids = torch.tensor(tokenizer.encode(prompt, add_special_tokens=False)).unsqueeze(0)\n",
    "    \n",
    "    #print(\"tokenization_ids\", tokenization_ids.shape)\n",
    "    #print(\"prompt_ids\", prompt_ids.shape)\n",
    "    input_ids = torch.cat((prompt_ids, tokenization_ids), dim=1) # Concatenate prompt and tokenization ids\n",
    "    #print(\"input_ids\", input_ids.shape)\n",
    "    # Get model's predictions (logits) for each token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Calculate the probability of the tokenization by multiplying the probabilities of each token\n",
    "    tokenization_probability = 1.0\n",
    "    tokenization_logit = 0\n",
    "    for idx, token in enumerate(tokenization):\n",
    "        # Get the token ID from the tokenizer\n",
    "        #print(\"token\", token)\n",
    "        #token_id = tokenizer.encode(token, add_special_tokens=False)[0]\n",
    "        token_id=token\n",
    "        \n",
    "        # Get the probability of the token in the model's output\n",
    "        token_probability = probabilities[0, prompt_ids.shape[1]+idx, token_id].item()\n",
    "        token_logit = logits[0, prompt_ids.shape[1]+idx, token_id].item()\n",
    "        tokenization_logit += token_logit\n",
    "        #print(\"current token prob\", token_probability)\n",
    "        #print(\"current token logit\", token_logit)\n",
    "        tokenization_probability *= token_probability\n",
    "\n",
    "    return tokenization_probability if not logit else tokenization_logit\n",
    "\n",
    "\n",
    "print(\"Initilizing script...\")\n",
    "custom_cache_dir = \"/NL/token-pricing/work/models\"\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=custom_cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             device_map=\"auto\",\n",
    "                                             cache_dir=custom_cache_dir)\n",
    "\n",
    "print(\"Model loaded...\")\n",
    "# Define the prompt and text\n",
    "prompt = \" \"\n",
    "text = \"Speechlessly\"\n",
    "\n",
    "# Find all possible tokenizations\n",
    "tokenizations = find_tokenizations(text, tokenizer, encode=True) #List, with each element=tokenization being a list of token IDs\n",
    "print(\"Tokenizations found...\")\n",
    "list_lengths = []\n",
    "list_prob = []\n",
    "\n",
    "for idx, tokenization in enumerate(tokenizations):\n",
    "    # Compute the probability of this tokenization\n",
    "    list_lengths.append(len(tokenization))\n",
    "    \n",
    "    prob = compute_tokenization_probability(tokenization, prompt, tokenizer, model, logit=False)\n",
    "    \n",
    "    list_prob.append(prob)\n",
    "    \n",
    "    #tokenization = [tokenizer.decode(tokenization, skip_special_tokens=True)\n",
    "                    \n",
    "    readable_tokenization = ' '.join(tokenizer.decode([token_id], skip_special_tokens=True) for token_id in tokenization)\n",
    "    \n",
    "    print(f\"Tokenization {idx + 1}: {readable_tokenization} | Probability/logit: {prob:.15f}\")\n",
    "\n",
    "list_prob = [prob / np.sum(list_prob) for prob in list_prob]  # Normalize the probabilities\n",
    "# Plot the KDE\n",
    "lengths, probs = sort_lists(torch.tensor(list_lengths, dtype=torch.float64), torch.tensor(list_prob, dtype=torch.float64))\n",
    "\n",
    "plot_kde(lengths, probs)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
