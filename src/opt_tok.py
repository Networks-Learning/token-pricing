import random
import matplotlib.pyplot as plt
from transformers import GPT2Tokenizer
import nltk


def optimal_tokenization(s, tokenizer, max_token_length=30):
    # Normalize the input string for compatibility with the tokenizer.
    s = s.replace("’", "'")  # Replace smart apostrophes with straight ones
    s = s.replace("‘", "'")  # Handle left single quotes as well
    s = tokenizer.convert_ids_to_tokens(tokenizer.encode(s, add_special_tokens=False))
    s = "".join(s).replace("Ġ", " ")  # Normalize spaces if applicable.

    # Build a normalized vocabulary.
    V = {}
    for token in tokenizer.get_vocab().keys():
        if token.startswith("Ġ"):
            normalized = " " + token[1:]
        else:
            normalized = token
        V[normalized] = token

    n = len(s)
    opt_counter = [float("inf")] * (n + 1)
    opt_counter[0] = 0

    # Array to track the optimal split points
    opt_index = [-1] * (n + 1)

    for i in range(1, n + 1):
        for j in range(max(0, i - max_token_length), i):
            substring = s[j:i]
            if substring in V:
                if opt_counter[j] + 1 < opt_counter[i]:
                    opt_counter[i] = opt_counter[j] + 1
                    opt_index[i] = j

    # Reconstruct the tokenization
    tokens = []
    i = n
    while i > 0:
        j = opt_index[i]
        token_str = s[j:i]
        if token_str in V:
            tokens.append(V[token_str])
        i = j

    tokens.reverse()

    # Format back to readable strings
    tokens = [token.replace("Ġ", " ") for token in tokens]

    # Generate token IDs and handle encoding errors gracefully
    ids = []
    for token in tokens:
        try:
            encoded = tokenizer.encode(token, add_special_tokens=False)
            if encoded:
                ids.append(encoded[0])
        except Exception as e:
            print(f"Error encoding token '{token}': {e}")

    return {"strings": tokens, "ids": ids}


def greedy_token_count(text, tokenizer):
    """Returns the number of tokens generated by GPT-2's built-in (greedy) tokenizer."""
    tokens = tokenizer.tokenize(text)
    return len(tokens)

def optimal_token_count(text, tokenizer):
    """Returns the number of tokens generated by our optimal tokenization routine."""
    tokens = optimal_tokenization(text, tokenizer)
    return len(tokens["strings"]) if tokens is not None else None



def optimal_tokenization2(s, tokenizer, max_token_length=30):
    # Normalize the input string for compatibility with the tokenizer.
    s = s.replace("’", "'").replace("‘", "'")  # Normalize quotes if necessary

    n = len(s)
    opt_counter = [float("inf")] * (n + 1)
    opt_counter[0] = 0

    # Array to track the optimal split points
    opt_index = [-1] * (n + 1)

    # Helper to check if a substring can be encoded as a single token
    def can_be_token(substring):
        encoded = tokenizer.encode(substring, add_special_tokens=False)
        if (len(encoded) == 1) or ():
            return True
        else:
            return False

    for i in range(1, n + 1):
        for j in range(max(0, i - max_token_length), i):
            substring = s[j:i]
            if can_be_token(substring):
                if opt_counter[j] + 1 < opt_counter[i]:
                    opt_counter[i] = opt_counter[j] + 1
                    opt_index[i] = j

    # Reconstruct the tokenization
    tokens = []
    i = n
    while i > 0:
        j = opt_index[i]
        token_str = s[j:i]
        tokens.append(token_str)
        i = j

    tokens.reverse()

    # Convert tokens to IDs
    ids = tokenizer.encode("".join(tokens), add_special_tokens=False)

    return {"strings": tokens, "ids": ids}
 
